{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOGXeXwQMw8qlLdJrtdUd7b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/limseo12/Breast-Ultrasound_CNN/blob/main/Breast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XXYfImbPfFT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dropout, Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdive/')"
      ],
      "metadata": {
        "id": "qJTTEZunQmQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROOT_DIR, DATA_ROOT_DIR 설정"
      ],
      "metadata": {
        "id": "xEmLDnyLFz3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "ROOT_DIR = '/content'\n",
        "\n",
        "DATA_ROOT_DIR = os.path.join(ROOT_DIR, 'Dataset_BUSI_with_GT')\n",
        "\n",
        "CLASSIFICATION_DATA_ROOT_DIR = os.path.join(ROOT_DIR, 'Classification')\n",
        "\n",
        "CLASSIFICATION_TRAIN_DATA_ROOT_DIR = os.path.join(CLASSIFICATION_DATA_ROOT_DIR, 'train')"
      ],
      "metadata": {
        "id": "JmhXTk_6RZBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "if os.path.exists(DATA_ROOT_DIR):\n",
        "  shutil.rmtree(DATA_ROOT_DIR)\n",
        "\n",
        "if os.path.exists(CLASSIFICATION_DATA_ROOT_DIR):\n",
        "  shutil.rmtree(CLASSIFICATION_DATA_ROOT_DIR)"
      ],
      "metadata": {
        "id": "KXR6oPEiRu_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive 에서 dataset download"
      ],
      "metadata": {
        "id": "tRB9R7zSF9Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "try:\n",
        "  dataset_path = '/content/gdive/MyDrive/dataset'\n",
        "\n",
        "  shutil.copy(os.path.join(dataset_path, 'Dataset_BUSI_with_GT.zip'), '/content')\n",
        "\n",
        "except Exception as err:\n",
        "  print*(str(err))"
      ],
      "metadata": {
        "id": "8iljjjz4QtSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import  zipfile\n",
        "\n",
        "with zipfile.ZipFile(os.path.join(ROOT_DIR, 'Dataset_BUSI_with_GT.zip'), 'r') as target_file:\n",
        "\n",
        "  target_file.extractall(DATA_ROOT_DIR)"
      ],
      "metadata": {
        "id": "MP3ghm5QR99h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "total_file_list = glob.glob(os.path.join(DATA_ROOT_DIR, '*'))\n",
        "\n",
        "label_name_list = [ file_name.split('/')[-1].strip() for file_name in total_file_list if os.path.isdir(file_name) == True]\n",
        "\n",
        "if not os.path.exists(CLASSIFICATION_DATA_ROOT_DIR):\n",
        "  os.mkdir(CLASSIFICATION_DATA_ROOT_DIR)\n",
        "\n",
        "for label_name in label_name_list:\n",
        "\n",
        "  src_dir_path = os.path.join(DATA_ROOT_DIR, label_name)\n",
        "  dst_dir_path = os.path.join(CLASSIFICATION_DATA_ROOT_DIR,\n",
        "                              'train' + '/' +label_name)\n",
        "  try:\n",
        "      shutil.copytree(src_dir_path, dst_dir_path)\n",
        "  except Exception as err:\n",
        "      print(str(err))\n",
        "\n",
        "train_label_name_list = os.listdir(CLASSIFICATION_TRAIN_DATA_ROOT_DIR)\n",
        "\n",
        "for label_name in train_label_name_list:\n",
        "  print('train label : ', label_name,' => ', len(os.listdir(os.path.join(CLASSIFICATION_TRAIN_DATA_ROOT_DIR, label_name))))"
      ],
      "metadata": {
        "id": "GNi_s7tnSWH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification 데이터 생성"
      ],
      "metadata": {
        "id": "nvzgPeGxGD5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(CLASSIFICATION_DATA_ROOT_DIR):\n",
        "    os.mkdir(CLASSIFICATION_DATA_ROOT_DIR)"
      ],
      "metadata": {
        "id": "i_xqY8PjGHn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copytree 이용해서 정답 이름/images 디렉토리를 train/정딥 이름 으로 복사\n",
        "\n",
        "for label_name in label_name_list:\n",
        "\n",
        "  src_dir_path = os.path.join(DATA_ROOT_DIR, label_name)\n",
        "  dst_dir_path = os.path.join(CLASSIFICATION_DATA_ROOT_DIR, 'train'+'/'+label_name)\n",
        "\n",
        "  try:\n",
        "    shutil.copytree(src_dir_path, dst_dir_path)\n",
        "    print(label_name+' copytree is done !!')\n",
        "\n",
        "  except Exception as err:\n",
        "    print(str(err))"
      ],
      "metadata": {
        "id": "0tfG61PhGSlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_label_name_list = os.listdir(CLASSIFICATION_TRAIN_DATA_ROOT_DIR)\n",
        "\n",
        "for label_name in train_label_name_list:\n",
        "\n",
        "  print('train label : ', label_name, ' => ', len(os.listdir(os.path.join(CLASSIFICATION_TRAIN_DATA_ROOT_DIR))))"
      ],
      "metadata": {
        "id": "uTUk4-6uG9OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "train_label_name_list = os.listdir(CLASSIFICATION_TRAIN_DATA_ROOT_DIR)\n",
        "\n",
        "for label_name in train_label_name_list:\n",
        "\n",
        "  temp_dic = {}\n",
        "\n",
        "  file_list = glob.glob(CLASSIFICATION_TRAIN_DATA_ROOT_DIR+'/'+label_name+'/*')\n",
        "\n",
        "  temp_dic[label_name] = file_list\n",
        "\n",
        "  temp_df = pd.DataFrame(temp_dic)\n",
        "\n",
        "  image_file_df = temp_df[-temp_df[label_name].str.contains('_mask')].reset_index(drop=True)\n",
        "\n",
        "  mask_file_df = temp_df[temp_df[label_name].str.contains('_mask')].reset_index(drop=True)\n",
        "\n",
        "  print('label =', label_name, ' , image = ', len(image_file_df), ' , mask = ', len(mask_file_df))\n",
        "\n",
        "  for row in range(len(mask_file_df)):\n",
        "\n",
        "      try:\n",
        "          os.remove(mask_file_df.loc[row.label_name])\n",
        "      except Exception as err:\n",
        "          print(str(err))"
      ],
      "metadata": {
        "id": "SUI-BtRDTbW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the Data - 학습 데이터 증강 (Data Augmentation)"
      ],
      "metadata": {
        "id": "N2lo4WSfg2RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "train_label_name_list = os.listdir(CLASSIFICATION_TRAIN_DATA_ROOT_DIR)\n",
        "\n",
        "total_image_data_nums = 0\n",
        "\n",
        "for label_name in train_label_name_list:\n",
        "\n",
        "  image_data_nums = len(os.listdir(os.path.join(CLASSIFICATION_TRAIN_DATA_ROOT_DIR, label_name)))\n",
        "\n",
        "  print('label = ', label_name, ' , data nums =', image_data_nums)\n",
        "\n",
        "  total_image_data_nums += image_data_nums\n",
        "\n",
        "print('total image data nums = ', total_image_data_nums)"
      ],
      "metadata": {
        "id": "MZePkL5pHbwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOTAL_TRAIN_IMAGE_DATA_NUMS = total_image_data_nums"
      ],
      "metadata": {
        "id": "efvQluUXWQcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_gen = ImageDataGenerator(rescale=1./255)\n",
        "original_generator = original_gen.flow_from_directory(CLASSIFICATION_TRAIN_DATA_ROOT_DIR,\n",
        "                                                      batch_size=TOTAL_TRAIN_IMAGE_DATA_NUMS, shuffle=False,\n",
        "                                                      target_size=(224,224), class_mode='sparse')"
      ],
      "metadata": {
        "id": "fwDYT5SPIS4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = original_generator.next()\n",
        "print(x_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "o_RukkSGIo3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentation Data 생성"
      ],
      "metadata": {
        "id": "PxMkOmvLI1UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUGMENTATION_COUNT = 4  #원본데이터의 4배 증대"
      ],
      "metadata": {
        "id": "wiI71h9MI3wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_gen = ImageDataGenerator(rescale=1./255, rotation_range=10, shear_range=0.1, zoom_range=0.1,\n",
        "                                      horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1)\n",
        "\n",
        "augmentation_generator = augmentation_gen.flow_from_directory(CLASSIFICATION_TRAIN_DATA_ROOT_DIR,\n",
        "                                    batch_size=TOTAL_TRAIN_IMAGE_DATA_NUMS, shuffle=False,\n",
        "                                    target_size=(224, 224), class_mode='sparse')"
      ],
      "metadata": {
        "id": "uRLzYALvI-w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print('====================================================')\n",
        "print('[bofore] ', x_train.shape, y_train.shape)\n",
        "print('====================================================')\n",
        "\n",
        "for i in range(AUGMENTATION_COUNT):    # 780개의 전체 데이터에 대해서 AUGMENTATION_COUNT 배 AUGMENTATION 실행\n",
        "\n",
        "    x_augmented, y_augmented = augmentation_generator.next()\n",
        "\n",
        "    x_train = np.concatenate( (x_train, x_augmented) )\n",
        "    y_train = np.concatenate( (y_train, y_augmented) )\n",
        "\n",
        "print('[after] ', x_train.shape, y_train.shape)\n",
        "print('====================================================')"
      ],
      "metadata": {
        "id": "I3De-PMLJkEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train data random shuffle"
      ],
      "metadata": {
        "id": "PEse5hK_KMLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = np.arange(x_train.shape[0])\n",
        "\n",
        "np.random.shuffle(s)\n",
        "\n",
        "x_train = x_train[s]\n",
        "y_train = y_train[s]"
      ],
      "metadata": {
        "id": "p9Xig9lMKOrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPLIT_RATIO = 0.2    # train : test = 8 : 2"
      ],
      "metadata": {
        "id": "rDvsPTCCKROg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_num = int(SPLIT_RATIO*(x_train.shape[0]))\n",
        "\n",
        "x_test = x_train[:split_num]\n",
        "\n",
        "y_test = y_train[:split_num]\n",
        "\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "rxExfdA6KSTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train[split_num:]\n",
        "\n",
        "y_train = y_train[split_num:]\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "HF7hECNeKTVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16 개의 데이터와 정답 출력"
      ],
      "metadata": {
        "id": "dsPpgxCEKUt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class_dict = {0:'benign', 1:'malignant', 2:'normal'}\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "for i in range(16):\n",
        "\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.title(str(class_dict[int(y_train[i])]))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.imshow(x_train[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DrEPmeEdKWfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE SHAPE 설정"
      ],
      "metadata": {
        "id": "vDAkmcdiKde_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224"
      ],
      "metadata": {
        "id": "_KiJUqPBKffI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential 모델 구축"
      ],
      "metadata": {
        "id": "Lm2vHjjmOM0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(input_shape=(32,32,3), kernel_size=(3,3), filters=32, activation='relu'))\n",
        "model.add(Conv2D(kernel_size=(3,3), filters=64, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(class_dict, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "aJuFb2sAOMOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "qG-5hu61Pbob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from datetime import datetime\n",
        "\n",
        "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "hist = model.fit(x_train, y_train, batch_size=32, epochs=30,\n",
        "                 validation_data=(x_test, y_test), callbacks=[earlystopping])\n",
        "\n",
        "end_time = datetime.now()\n",
        "\n",
        "print('elapsed time => ', end_time-start_time)"
      ],
      "metadata": {
        "id": "Fm1rjwkrPmY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "0NdcDemFP4yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(y_pred.shape)"
      ],
      "metadata": {
        "id": "Bl4FQOKgP53N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(hist.history['accuracy'], label='train')\n",
        "plt.plot(hist.history['val_accuracy'], label='validation')\n",
        "plt.title('Accuracy Trend')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kELq0dt8P7Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history['loss'], label='train')\n",
        "plt.plot(hist.history['val_loss'], label='validation')\n",
        "plt.title('Loss Trend')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iETU-DiVP8EX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}