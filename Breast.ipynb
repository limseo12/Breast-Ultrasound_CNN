{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjbe8D4SRARZC/Jiz5ANh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/limseo12/Breast-Ultrasound_CNN/blob/main/Breast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XXYfImbPfFT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dropout, Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "qJTTEZunQmQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROOT_DIR, DATA_ROOT_DIR 설정"
      ],
      "metadata": {
        "id": "xEmLDnyLFz3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "ROOT_DIR = '/content'\n",
        "\n",
        "DATA_ROOT_DIR = os.path.join(ROOT_DIR, 'Dataset_BUSI_with_GT')"
      ],
      "metadata": {
        "id": "JmhXTk_6RZBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSIFICATION_DATA_ROOT_DIR = os.path.join(ROOT_DIR, 'Classification') #classification\n",
        "\n",
        "CLASSIFICATION_TRAIN_DATA_ROOT_DIR = os.path.join(CLASSIFICATION_DATA_ROOT_DIR, 'train')  #train"
      ],
      "metadata": {
        "id": "Vfpey4Y-VVD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(DATA_ROOT_DIR):\n",
        "    #if os.path.exists(DATA_ROOT_DIR):: os.path.exists() 함수를 사용하여 DATA_ROOT_DIR 경로가 존재하는지 확인합니다.\n",
        "    shutil.rmtree(DATA_ROOT_DIR)\n",
        "    #shutil.rmtree(DATA_ROOT_DIR): shutil.rmtree() 함수를 사용하여 DATA_ROOT_DIR 경로와 해당 디렉토리의 하위 파일 및 디렉토리를 모두 삭제합니다.\n",
        "    #디렉토리가 비어있지 않은 경우에도 디렉토리와 하위 파일 및 디렉토리를 모두 삭제합니다. 즉, DATA_ROOT_DIR 디렉토리를 완전히 제거합니다.\n",
        "if os.path.exists(CLASSIFICATION_DATA_ROOT_DIR):\n",
        "    #if os.path.exists(CLASSIFICATION_DATA_ROOT_DIR):: os.path.exists() 함수를 사용하여 CLASSIFICATION_DATA_ROOT_DIR 경로가 존재하는지 확인합니다.\n",
        "    shutil.rmtree(CLASSIFICATION_DATA_ROOT_DIR)\n",
        "    #shutil.rmtree(CLASSIFICATION_DATA_ROOT_DIR): shutil.rmtree() 함수를 사용하여 CLASSIFICATION_DATA_ROOT_DIR 경로와 해당 디렉토리의 하위 파일 및 디렉토리를 모두 삭제합니다.\n",
        "    #마찬가지로 디렉토리가 비어있지 않은 경우에도 디렉토리와 하위 파일 및 디렉토리를 모두 삭제합니다. CLASSIFICATION_DATA_ROOT_DIR 디렉토리를 완전히 제거합니다.\n",
        "    #즉, 위의 코드는 DATA_ROOT_DIR와 CLASSIFICATION_DATA_ROOT_DIR 디렉토리가 이미 존재한다면 해당 디렉토리를 삭제하는 역할을 수행합니다.\n",
        "    #이전에 생성되었던 데이터를 삭제하고 새로운 데이터를 다시 생성하기 위해 해당 디렉토리를 비우는 작업을 수행합니다."
      ],
      "metadata": {
        "id": "KXR6oPEiRu_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive 에서 dataset download"
      ],
      "metadata": {
        "id": "tRB9R7zSF9Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  dataset_path = '/content/gdrive/MyDrive/dataset'\n",
        "\n",
        "  shutil.copy(os.path.join(dataset_path, 'Dataset_BUSI_with_GT.zip'), '/content')\n",
        "\n",
        "except Exception as err:\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "8iljjjz4QtSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import  zipfile\n",
        "\n",
        "with zipfile.ZipFile(os.path.join(ROOT_DIR, 'Dataset_BUSI_with_GT.zip'), 'r') as target_file:\n",
        "\n",
        "  target_file.extractall(DATA_ROOT_DIR)"
      ],
      "metadata": {
        "id": "MP3ghm5QR99h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "total_file_list = glob.glob(os.path.join('/content/Dataset_BUSI_with_GT/Dataset_BUSI_with_GT', '*'))\n",
        "\n",
        "label_name_list = [ file_name.split('/')[-1].strip() for file_name in total_file_list if os.path.isdir(file_name) == True]\n",
        "#label_name_list = [file_name.split('/')[-1].strip() for file_name in total_file_list if os.path.isdir(file_name) == True]: total_file_list에서 디렉토리 경로만 추출하여 label_name_list에 저장합니다.\n",
        "#각 경로에서 마지막 디렉토리 이름을 추출하기 위해 split('/')을 사용하고, 추출한 이름에서 양쪽 공백을 제거하기 위해 strip()을 사용합니다.\n",
        "#os.path.isdir(file_name) == True 조건을 사용하여 디렉토리인지 확인합니다.\n",
        "\n",
        "if not os.path.exists(CLASSIFICATION_DATA_ROOT_DIR):\n",
        "  os.mkdir(CLASSIFICATION_DATA_ROOT_DIR)\n",
        "\n",
        "#if not os.path.exists(CLASSIFICATION_DATA_ROOT_DIR): os.mkdir(CLASSIFICATION_DATA_ROOT_DIR): CLASSIFICATION_DATA_ROOT_DIR 경로가 존재하지 않으면 해당 디렉토리를 생성합니다.\n",
        "#분류 작업에 사용될 디렉토리를 생성하는 역할을 합니다.\n",
        "for label_name in label_name_list:\n",
        "#for label_name in label_name_list: ...: label_name_list의 각 항목에 대해 반복합니다. 각 레이블 이름에 대해 다음 작업을 수행합니다:\n",
        "  src_dir_path = os.path.join('/content/Dataset_BUSI_with_GT/Dataset_BUSI_with_GT', label_name)\n",
        "#src_dir_path = os.path.join(DATA_ROOT_DIR, label_name): 소스 디렉토리 경로를 DATA_ROOT_DIR과 label_name을 결합하여 생성합니다.\n",
        "  dst_dir_path = os.path.join(CLASSIFICATION_DATA_ROOT_DIR,\n",
        "                              'train' + '/' +label_name)\n",
        "#dst_dir_path = os.path.join(CLASSIFICATION_DATA_ROOT_DIR, 'train'+'/'+label_name): 대상 디렉토리 경로를 CLASSIFICATION_DATA_ROOT_DIR, 'train', 그리고 label_name을 결합하여 생성합니다.\n",
        "# 이렇게 함으로써 데이터를 분류하여 저장할 디렉토리 경로를 생성합니다.\n",
        "  try:\n",
        "      shutil.copytree(src_dir_path, dst_dir_path)\n",
        "  except Exception as err:\n",
        "      print(str(err))\n",
        "#shutil.copytree(src_dir_path, dst_dir_path): src_dir_path에서 dst_dir_path로 디렉토리와 파일을 복사합니다. 소스 디렉토리의 모든 내용을 대상 디렉토리로 복사합니다.\n",
        "\n",
        "train_label_name_list = os.listdir(CLASSIFICATION_TRAIN_DATA_ROOT_DIR)\n",
        "#train_label_name_list = os.listdir(CLASSIFICATION_TRAIN_DATA_ROOT_DIR): CLASSIFICATION_TRAIN_DATA_ROOT_DIR 디렉토리에 있는 디렉토리 목록을 가져와 train_label_name_list에 저장합니다.\n",
        "for label_name in train_label_name_list:\n",
        "  print('train label : ', label_name,' => ', len(os.listdir(os.path.join(CLASSIFICATION_TRAIN_DATA_ROOT_DIR, label_name))))\n",
        "\n",
        "#for label_name in train_label_name_list: ...: train_label_name_list의 각 항목에 대해 반복합니다. 각 레이블 이름에 대해 다음 작업을 수행합니다:\n",
        "\n",
        "#os.path.join(CLASSIFICATION_TRAIN_DATA_ROOT_DIR, label_name): CLASSIFICATION_TRAIN_DATA_ROOT_DIR과 label_name을 결합하여 디렉토리의 경로를 생성합니다.\n",
        "#os.listdir(os.path.join(CLASSIFICATION_TRAIN_DATA_ROOT_DIR, label_name)): 해당 디렉토리 안에 있는 파일과 디렉토리의 목록을 가져와 출력합니다. 이는 해당 레이블의 학습 데이터 수를 확인하는 용도로 사용됩니다.\n",
        "#위의 코드는 데이터 디렉토리의 파일을 분류하여 학습에 사용할 수 있도록 준비하는 작업을 수행합니다. 분류된 데이터는 CLASSIFICATION_TRAIN_DATA_ROOT_DIR 디렉토리에 복사되며, 각 레이블에 대한 학습 데이터의 수를 출력하여 확인합니다."
      ],
      "metadata": {
        "id": "GNi_s7tnSWH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification 데이터 생성"
      ],
      "metadata": {
        "id": "nvzgPeGxGD5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(CLASSIFICATION_DATA_ROOT_DIR):\n",
        "    os.mkdir(CLASSIFICATION_DATA_ROOT_DIR)"
      ],
      "metadata": {
        "id": "i_xqY8PjGHn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copytree 이용해서 정답 이름/images 디렉토리를 train/정딥 이름 으로 복사\n",
        "\n",
        "for label_name in label_name_list:\n",
        "\n",
        "  src_dir_path = os.path.join('/content/Dataset_BUSI_with_GT/Dataset_BUSI_with_GT', label_name)\n",
        "  dst_dir_path = os.path.join(CLASSIFICATION_DATA_ROOT_DIR, 'train'+'/'+label_name)\n",
        "\n",
        "  try:\n",
        "    shutil.copytree(src_dir_path, dst_dir_path)\n",
        "    print(label_name+' copytree is done !!')\n",
        "\n",
        "  except Exception as err:\n",
        "    print(str(err))"
      ],
      "metadata": {
        "id": "0tfG61PhGSlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label_name_list = os.listdir(CLASSIFICATION_TRAIN_DATA_ROOT_DIR)\n",
        "\n",
        "for label_name in train_label_name_list:\n",
        "\n",
        "  print('train label : ', label_name, ' => ', len(os.listdir(os.path.join(CLASSIFICATION_TRAIN_DATA_ROOT_DIR))))"
      ],
      "metadata": {
        "id": "uTUk4-6uG9OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "train_label_name_list = os.listdir(CLASSIFICATION_TRAIN_DATA_ROOT_DIR)\n",
        "\n",
        "for label_name in train_label_name_list:\n",
        "\n",
        "  temp_dic = {}\n",
        "\n",
        "  file_list = glob.glob(CLASSIFICATION_TRAIN_DATA_ROOT_DIR+'/'+label_name+'/*')\n",
        "\n",
        "  temp_dic[label_name] = file_list\n",
        "\n",
        "  temp_df = pd.DataFrame(temp_dic)\n",
        "\n",
        "  image_file_df = temp_df[-temp_df[label_name].str.contains('_mask')].reset_index(drop=True)\n",
        "\n",
        "  mask_file_df = temp_df[temp_df[label_name].str.contains('_mask')].reset_index(drop=True)\n",
        "\n",
        "  print('label =', label_name, ' , image = ', len(image_file_df), ' , mask = ', len(mask_file_df))\n",
        "\n",
        "  for row in range(len(mask_file_df)):\n",
        "\n",
        "      try:\n",
        "          os.remove(mask_file_df.loc[row.label_name])\n",
        "      except Exception as err:\n",
        "          print(str(err))"
      ],
      "metadata": {
        "id": "SUI-BtRDTbW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the Data - 학습 데이터 증강 (Data Augmentation)"
      ],
      "metadata": {
        "id": "N2lo4WSfg2RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_label_name_list = os.listdir(CLASSIFICATION_TRAIN_DATA_ROOT_DIR)\n",
        "\n",
        "total_image_data_nums = 0\n",
        "\n",
        "for label_name in train_label_name_list:\n",
        "\n",
        "  image_data_nums = len(os.listdir(os.path.join(CLASSIFICATION_TRAIN_DATA_ROOT_DIR, label_name)))\n",
        "\n",
        "  print('label = ', label_name, ' , data nums =', image_data_nums)\n",
        "\n",
        "  total_image_data_nums += image_data_nums\n",
        "\n",
        "print('total image data nums = ', total_image_data_nums)"
      ],
      "metadata": {
        "id": "MZePkL5pHbwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOTAL_TRAIN_IMAGE_DATA_NUMS = total_image_data_nums"
      ],
      "metadata": {
        "id": "efvQluUXWQcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_gen = ImageDataGenerator(rescale=1./255) #불러오면서 정규화\n",
        "original_generator = original_gen.flow_from_directory(CLASSIFICATION_TRAIN_DATA_ROOT_DIR,\n",
        "                                                      batch_size=TOTAL_TRAIN_IMAGE_DATA_NUMS, shuffle=False,\n",
        "                                                      target_size=(224,224), class_mode='sparse')"
      ],
      "metadata": {
        "id": "fwDYT5SPIS4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = original_generator.next()\n",
        "print(x_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "o_RukkSGIo3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentation Data 생성"
      ],
      "metadata": {
        "id": "PxMkOmvLI1UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUGMENTATION_COUNT = 4  #원본데이터의 4배 증대"
      ],
      "metadata": {
        "id": "wiI71h9MI3wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_gen = ImageDataGenerator(rescale=1./255, rotation_range=10, shear_range=0.1, zoom_range=0.1,\n",
        "                                      horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1)\n",
        "\n",
        "augmentation_generator = augmentation_gen.flow_from_directory(CLASSIFICATION_TRAIN_DATA_ROOT_DIR,\n",
        "                                    batch_size=TOTAL_TRAIN_IMAGE_DATA_NUMS, shuffle=False,\n",
        "                                    target_size=(224, 224), class_mode='sparse')"
      ],
      "metadata": {
        "id": "uRLzYALvI-w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print('====================================================')\n",
        "print('[bofore] ', x_train.shape, y_train.shape)\n",
        "print('====================================================')\n",
        "\n",
        "for i in range(AUGMENTATION_COUNT):    # 780개의 전체 데이터에 대해서 AUGMENTATION_COUNT 배 AUGMENTATION 실행\n",
        "\n",
        "    x_augmented, y_augmented = augmentation_generator.next()\n",
        "\n",
        "    x_train = np.concatenate( (x_train, x_augmented) )\n",
        "    y_train = np.concatenate( (y_train, y_augmented) )\n",
        "\n",
        "print('[after] ', x_train.shape, y_train.shape)\n",
        "print('====================================================')"
      ],
      "metadata": {
        "id": "I3De-PMLJkEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train data random shuffle"
      ],
      "metadata": {
        "id": "PEse5hK_KMLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = np.arange(x_train.shape[0])\n",
        "\n",
        "np.random.shuffle(s)\n",
        "\n",
        "x_train = x_train[s]\n",
        "y_train = y_train[s]"
      ],
      "metadata": {
        "id": "p9Xig9lMKOrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPLIT_RATIO = 0.2    # train : test = 8 : 2"
      ],
      "metadata": {
        "id": "rDvsPTCCKROg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_num = int(SPLIT_RATIO*(x_train.shape[0]))\n",
        "\n",
        "x_test = x_train[:split_num]\n",
        "\n",
        "y_test = y_train[:split_num]\n",
        "\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "rxExfdA6KSTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train[split_num:]\n",
        "\n",
        "y_train = y_train[split_num:]\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "HF7hECNeKTVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16 개의 데이터와 정답 출력"
      ],
      "metadata": {
        "id": "dsPpgxCEKUt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class_dict = {0:'benign', 1:'malignant', 2:'normal'}\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "for i in range(16):\n",
        "\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.title(str(class_dict[int(y_train[i])]))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.imshow(x_train[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DrEPmeEdKWfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE SHAPE 설정"
      ],
      "metadata": {
        "id": "vDAkmcdiKde_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224"
      ],
      "metadata": {
        "id": "_KiJUqPBKffI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential 모델 구축"
      ],
      "metadata": {
        "id": "Lm2vHjjmOM0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import xception\n",
        "\n",
        "class_nums = original_generator.num_classes\n",
        "##Transfer Learning##\n",
        "pre_trained_model = xception(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
        "pre_trained_model.summary()\n",
        "#####################\n",
        "model = Sequential()\n",
        "####################\n",
        "model.add(pre_trained_model)\n",
        "############################\n",
        "model.add(Conv2D(input_shape=(IMG_WIDTH,IMG_HEIGHT,3), kernel_size=(3,3), filters=32, activation='relu'))  #이미지 사이즈\n",
        "model.add(Conv2D(kernel_size=(3,3), filters=64, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(class_nums, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "aJuFb2sAOMOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "qG-5hu61Pbob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from datetime import datetime\n",
        "\n",
        "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "hist = model.fit(x_train, y_train, batch_size=32, epochs=50,\n",
        "                 validation_data=(x_test, y_test), callbacks=[earlystopping])\n",
        "\n",
        "end_time = datetime.now()\n",
        "\n",
        "print('elapsed time => ', end_time-start_time)"
      ],
      "metadata": {
        "id": "Fm1rjwkrPmY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "0NdcDemFP4yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(y_pred.shape)"
      ],
      "metadata": {
        "id": "Bl4FQOKgP53N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(hist.history['accuracy'], label='train')\n",
        "plt.plot(hist.history['val_accuracy'], label='validation')\n",
        "plt.title('Accuracy Trend')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kELq0dt8P7Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history['loss'], label='train')\n",
        "plt.plot(hist.history['val_loss'], label='validation')\n",
        "plt.title('Loss Trend')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iETU-DiVP8EX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}